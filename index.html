<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta property="og:title" content="All Things ViTs: Understanding and Interpreting Attention in Vision"/>
  <meta property="og:url" content="https://attendandexcite.github.io/attend-and-excite/"/>
  <meta property="og:image" content="static/images/og_tag_header_image.jpg" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <title>All Things ViTs</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">
  <link rel="icon" href="static/figures/elephant.jpeg">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


<section class="publication-header">
  <div class="hero-body">
    <div class="container is-max-widescreen">
      <!-- <div class="columns is-centered"> -->
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">All Things ViTs: Understanding and Interpreting Attention in Vision</h1>
        </div>
    </div>
  </div>   
</section>

<section class="publication-author-block">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
		<div class="is-size-4 publication-authors">
            <span class="author-block"><a href="https://hila-chefer.github.io/" target="_blank">Hila Chefer</a><sup>1</sup>,</span>
			<span class="author-block"><a href="https://sayak.dev/" target="_blank">Sayak Paul</a><sup>2</sup></span>
          </div>
		  <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>1</sup>Tel Aviv University, Google <sup>2</sup>Hugging Face
            <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              
              <!--span class="link-block">
                <a href="https://arxiv.org/abs/2301.13826" target="_blank"
                  class="external-link button is-normal is-rounded">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span -->
              
              <span class="link-block">
                <a href="https://github.com/all-things-vits" target="_blank"
                class="external-link button is-normal is-rounded">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
              </span>
             <!--span class="link-block">
              <a href=""  target="_blank"
                 class="external-link button is-normal is-rounded">
                <span class="icon">
                    <i class="fas fa-infinity"></i>
                </span>
                <span>Colab</span>
              </a>
             </span -->
            
            <span class="link-block">
              <a href="https://huggingface.co/all-things-vits"  target="_blank"
                 class="external-link button is-normal is-rounded">
                <span class="icon">
                    <i class="fas fa-laptop"></i>
                </span>
                <span>Demo</span>
              </a>
             </span>
           
            </div>
          </div>

        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <!-- <div class="hero-body"> -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <div id="results-carousel" class="carousel results-carousel"> -->
      <div class="container">
      <div class="item">
      <div class="column is-centered has-text-centered">
        <img src="static/figures/teaser.jpg" alt="All_Things_ViTs" width="750px"/>
      
      <h2 class="subtitle">
	In this tutorial, we explore different ways to leverage attention in vision. From left to right: (i) attention can be used to explain the predictions by the model (e.g., CLIP for an image-text pair) (ii) By manipulating the attention-based explainability maps, one can enforce that the prediction is made based on the right reasons (e.g., foreground vs. background) (iii) The cross-attention maps of multi-modal models can be used to guide generative models (e.g., mitigating neglect in Stable Diffusion).

      </h2>
	  </div>
    </div>
  </div>
 <!--  </div> -->
  </div>
  </div>
 <!--  </div> -->
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The attention mechanism has revolutionized deep learning research across many disciplines, starting from NLP and expanding to vision, speech, and more. Different from other mechanisms, the elegant and general attention mechanism is easily adaptable and eliminates modality-specific inductive biases. As attention become increasingly popular, it is crucial to develop tools that allow researchers to understand and explain the inner workings of the mechanism to facilitate better and more responsible use of it. This tutorial focuses on understanding and interpreting attention in the vision, and in the multi-modal settings combining text and vision. We present state-of-the-art research on representation probing, interpretability, and attention-based semantic guidance, alongside hands-on demos to facilitate interactivity. Additionally, we discuss open questions arising from recent works and future research directions.

          </p>
        </div>
        <h2 class="title is-3">Tutorial Outline</h2>
        <div class="content has-text-justified">
          <p>
            The following is a tentative outline of the tutorial. We will publish the final outline soon.
            <ul>
              <li>
                  <p>Overview of typical ways of interpreting CNNs - GradCAM, LRP, grad x input, SHAP, LIME, Integrated Gradients</p>
              </li>
              <li>Short introduction to Transformers
                  <ul>
                      <li>
                          <p>The attention mechanism</p>
                      </li>
                      <li>
                          <p>Positional encoding</p>
                      </li>
                      <li>
                          <p>Integrating attention maps from different modalities via cross-attention</p>
                      </li>
                  </ul>
              </li>
              <li>Probing Transformers: understanding what Transformers learn from images [<a href="https://arxiv.org/abs/2010.11929">2</a>,&nbsp;<a href="https://arxiv.org/abs/2108.08810">3</a>,&nbsp;<a href="https://arxiv.org/abs/2103.17239">4</a>,&nbsp;<a href="https://arxiv.org/abs/2202.06709">5</a>]
                  <ul>
                      <li>
                          <p>Mean attention distance (relative receptive field)</p>
                      </li>
                      <li>
                          <p>Centered kernel alignment&nbsp;</p>
                      </li>
                      <li>
                          <p>The role of skip connections</p>
                      </li>
                  </ul>
              </li>
              <li>Why do we need different methods to interpret Transformers?
                  <ul>
                      <li>
                          <p>Is attention an explanation? If so, under what conditions?</p>
                      </li>
                  </ul>
              </li>
              <li>Explaining predictions made by Transformers (XAI)
                  <ul>
                      <li>Algorithms to explain attention
                          <ul>
                              <li>
                                  <p>Attention rollout [<a href="https://arxiv.org/abs/2005.00928">1</a>]</p>
                              </li>
                              <li>
                                  <p>Attention flow [<a href="https://arxiv.org/abs/2005.00928">1</a>]</p>
                              </li>
                              <li>
                                  <p>Transformer Interpretability Beyond Attention Visualization [<a href="https://arxiv.org/abs/2012.09838">6</a>]</p>
                              </li>
                              <li>
                                  <p>Understanding what Transformers learn from multiple modalities [<a href="https://arxiv.org/abs/2103.15679">7</a>]</p>
                              </li>
                          </ul>
                      </li>
                      <li>Attention as explanation
                          <ul>
                              <li>
                                  <p>Class attention [<a href="https://arxiv.org/abs/2103.17239">4</a>]</p>
                              </li>
                              <li>
                                  <p>Attention for semantic segmentation (DINO [<a href="https://arxiv.org/abs/2104.14294">8</a>])</p>
                              </li>
                          </ul>
                      </li>
                  </ul>
              </li>
              <li>Open questions
                  <ul>
                      <li>
                          <p>How do we evaluate these explainability methods?&nbsp;</p>
                      </li>
                      <li>
                          <p>Are smaller Transformer models better than the larger ones as far as explainability is concerned?&nbsp;</p>
                      </li>
                      <li>
                          <p>Is attention a good way to interpret Transformers in the first place?&nbsp;</p>
                      </li>
                      <li>
                          <p>Why do methods for CNNs not perform well on Transformers? (e.g. the adaptation of GradCAM to Transformers does not seem to work, even though the intuition is rather similar)</p>
                      </li>
                  </ul>
              </li>
              <li>
                  <p>Conclusion and Q&amp;A</p>
              </li>
          </ul>
          </p>
        </div>
        <h2 class="title is-3">Logistics</h2>
        <div class="content has-text-justified">
          <p>
            Coming soon.

          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<footer class="footer">
  <div class="columns is-centered">
    <div class="column is-8">
      <div class="content">
        <p>
          This website is licensed under a <a rel="license"
          href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
        Commons Attribution-ShareAlike 4.0 International License</a>.
      </p>
      <p>
        Website source code based on the <a href="https://nerfies.github.io/"> Nerfies</a> project page. If you want to reuse their <a
        href="https://github.com/nerfies/nerfies.github.io">source code</a>, please credit them appropriately.
      </p>
    </div>
  </div>
</div>
</div>
</footer>


  <script type="text/javascript">
    var sc_project=12351448; 
    var sc_invisible=1; 
    var sc_security="c676de4f"; 
  </script>
  <script type="text/javascript"
  src="https://www.statcounter.com/counter/counter.js"
  async></script>
  <noscript><div class="statcounter"><a title="Web Analytics"
    href="https://statcounter.com/" target="_blank"><img
    class="statcounter"
    src="https://c.statcounter.com/12351448/0/c676de4f/1/"
    alt="Web Analytics"></a></div></noscript>
    <!-- End of Statcounter Code -->

  </body>
  </html>